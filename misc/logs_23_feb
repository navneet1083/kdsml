root@fe7d646c6686:~/projects/repos/kdsml# torchrun --nproc_per_node=4 ddp_main.py
W0223 10:44:19.921000 140271701890880 torch/distributed/run.py:779]
W0223 10:44:19.921000 140271701890880 torch/distributed/run.py:779] *****************************************
W0223 10:44:19.921000 140271701890880 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0223 10:44:19.921000 140271701890880 torch/distributed/run.py:779] *****************************************
/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [25:52<00:00,  1.13batch/s, Iter=1760/1760, Loss=1.0696]
Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [25:51<00:00,  1.13batch/s, Iter=1760/1760, Loss=0.6303]

Epoch 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [25:51<00:00,  1.13batch/s, Iter=1760/1760, Loss=1.0594]
Epoch 2/10:   0%|                                                                                                                                                                                                                                                                               | 0/1760 [00:00<?, ?batch/s]
Epoch 1 Summary:
  Train Loss: 1.2018
  Val Loss:   1.0638
  Val Acc:    0.00%
Checkpoint saved for epoch 1

Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [26:05<00:00,  1.12batch/s, Iter=1760/1760, Loss=1.0342]
Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [26:06<00:00,  1.12batch/s, Iter=1760/1760, Loss=1.2629]
Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [26:05<00:00,  1.12batch/s, Iter=1760/1760, Loss=0.7074]
Epoch 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1760/1760 [26:00<00:00,  1.13batch/s, Iter=1760/1760, Loss=0.6921]
Epoch 3/10:   0%|                                                                                                                                                                                                                                                                               | 0/1760 [00:00<?, ?batch/s]
Epoch 2 Summary:
  Train Loss: 1.0068
  Val Loss:   0.9443
  Val Acc:    0.00%
Checkpoint saved for epoch 2

Epoch 3/10:  33%|████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                            | 580/1760 [08:41<17:31,  1.12batch/s, Iter=580/1760, Loss=0.9246]
