{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad18dd0a-bbc5-47cd-9fd1-132698472e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"/Users/navneetsingh/Library/Mobile Documents/com~apple~CloudDocs/Work/GitHub-Repos/personal_repos/BITS/kd-slm-training\")\n",
    "os.chdir(\"/home/work/repos/4_02_2025/kdsml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662b0c77-eeb8-4d87-a62b-6fb1c5cc1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from models.teacher_model import TeacherModel\n",
    "from models.student_model import StudentModel\n",
    "from utils.loss_functions import KnowledgeDistillationLoss\n",
    "from training.trainer import Trainer\n",
    "from training.validator import Validator\n",
    "from evaluation.evaluator import Evaluator\n",
    "from utils.data_loader import DataLoader\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafeddd9-9284-44ae-94e1-59f325727c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize models, loss, and optimizer\n",
    "teacher_model = TeacherModel().to('cuda')\n",
    "student_model = StudentModel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7a2262-955a-4a84-818f-c219e121a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = KnowledgeDistillationLoss().to('cuda')  # Move loss function to GPU\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef54d3f-6e5f-49c4-804c-137f733fcbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer and validator\n",
    "trainer = Trainer(teacher_model, student_model, loss_fn, optimizer, 'cuda')\n",
    "validator = Validator(student_model, loss_fn, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12da101d-3233-4b35-ae61-2207300846a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data (WikiText-103)\n",
    "train_loader = DataLoader(dataset_name='wikitext').get_dataloader(split='train', batch_size=256)\n",
    "val_loader = DataLoader(dataset_name='wikitext').get_dataloader(split='validation', batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ee131e-05df-432a-a311-cda15d627df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess evaluation data (SQuAD)\n",
    "# eval_loader = DataLoader(dataset_name='squad').get_dataloader(split='validation', batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cbcf3b7-5271-4ed0-94c9-284e4cec23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dummy Forward Pass Test ---\n",
    "def test_dummy_forward():\n",
    "    # Create dummy inputs that mimic a real batch\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    vocab_size = 30522\n",
    "\n",
    "    # Dummy input_ids and attention_mask on device\n",
    "    dummy_input_ids = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "    dummy_attention_mask = torch.ones_like(dummy_input_ids).to(device)\n",
    "    # Create dummy labels by shifting the input_ids (simulating language modeling)\n",
    "    dummy_labels = dummy_input_ids.clone()\n",
    "    dummy_labels[:, :-1] = dummy_input_ids[:, 1:]\n",
    "    dummy_labels[:, -1] = 0  # Assume pad token id is 0\n",
    "\n",
    "    # Teacher forward pass (ensure outputs are on the same device)\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(dummy_input_ids, dummy_attention_mask)\n",
    "    teacher_hidden_states = teacher_outputs['last_hidden_state']\n",
    "\n",
    "    # Student forward pass\n",
    "    student_logits = student_model(dummy_input_ids, dummy_attention_mask)\n",
    "\n",
    "    # Compute loss to verify that all tensors are on the same device\n",
    "    loss_val = loss_fn(student_logits, teacher_hidden_states, dummy_labels)\n",
    "    print(\"Dummy forward pass successful. Loss:\", loss_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e963ff-a6e1-4fba-93c5-92ff8d6987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "num_epochs = 10\n",
    "checkpoint_path = os.path.join(\"checkpoints\", \"checkpoint.pth\")\n",
    "device = 'cuda'\n",
    "\n",
    "# Initialize metrics containers\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5adf1fe5-550e-4670-9a60-e97835e7f268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New run with new weights\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    student_model.load_state_dict(checkpoint['student_model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    val_accuracies = checkpoint.get('val_accuracies', [])\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(f'New run with new weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bf5c1e-72df-460e-91c9-1f59c6aff72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|‚ñè         | 95/7037 [02:54<3:32:48,  1.84s/batch, Iter=95/7037, Loss=1.3828]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Perform a training step and update loss\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update progress bar with current iteration loss\u001b[39;00m\n",
      "File \u001b[0;32m~/work/repos/4_02_2025/kdsml/training/trainer.py:60\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(\"Student Outputs Shape:\", student_outputs.shape)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print(\"Teacher Hidden States Shape:\", teacher_hidden_states.shape)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# print(\"Labels Shape:\", labels.shape)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    student_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    epoch_steps = len(train_loader)\n",
    "    \n",
    "    # Progress bar for batches in the epoch\n",
    "    pbar = tqdm(enumerate(train_loader), total=epoch_steps, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for i, batch in pbar:\n",
    "        # Ensure batch tensors are on the correct device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Perform a training step and update loss\n",
    "        train_loss = trainer.train_step(input_ids, attention_mask, labels)\n",
    "        total_train_loss += train_loss\n",
    "        \n",
    "        # Update progress bar with current iteration loss\n",
    "        pbar.set_postfix({\n",
    "            'Iter': f\"{i+1}/{epoch_steps}\",\n",
    "            'Loss': f\"{train_loss:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    avg_train_loss = total_train_loss / epoch_steps\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    # We assume that validator.validate returns a tuple: (loss, accuracy)\n",
    "    val_loss, val_accuracy = validator.validate(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Print epoch summary with additional metrics\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  Val Acc:    {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint including current epoch, model, optimizer, and metric histories\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'student_model_state_dict': student_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1} to '{checkpoint_path}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37f8c0-c49d-4fde-8514-f62c07234b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses, val_losses = [], []\n",
    "# for epoch in range(10):\n",
    "#     # Train on WikiText-103\n",
    "#     total_train_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         input_ids, attention_mask, labels = (\n",
    "#             batch['input_ids'].to('cuda'),\n",
    "#             batch['attention_mask'].to('cuda'),\n",
    "#             batch['labels'].to('cuda')\n",
    "#         )\n",
    "#         train_loss = trainer.train_step(input_ids, attention_mask, labels)\n",
    "#         total_train_loss += train_loss\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     train_losses.append(avg_train_loss)\n",
    "\n",
    "#     # Validate on WikiText-103\n",
    "#     val_loss = validator.validate(val_loader)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss}, Val Loss = {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd1344-3c19-4614-b8e3-fbf86b380b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb76efa-7b49-4798-997a-ee8a4e90e9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549990b-6a50-4217-807b-cb164bfaaaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
